{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "base_url = 'https://api.energy-charts.info/'\n",
    "country_code = 'ch'\n",
    "\n",
    "def process_timestamps(unix_seconds):\n",
    "    return [datetime.utcfromtimestamp(ts) for ts in unix_seconds]\n",
    "\n",
    "def get_public_power():\n",
    "    start_date = date(2010, 1, 1)\n",
    "    end_date = date(2024, 4, 23)\n",
    "\n",
    "    start_datetime = start_date.strftime('%Y-%m-%dT%H:%M:%S%z')\n",
    "    end_datetime = end_date.strftime('%Y-%m-%dT%H:%M:%S%z')\n",
    "    \n",
    "    url = base_url + 'public_power'\n",
    "    params = {\n",
    "        'country': country_code,\n",
    "        'start': start_datetime,\n",
    "        'end': end_datetime\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve data: {response.status_code} - {response.text}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = response.json()\n",
    "    timestamps = process_timestamps(data['unix_seconds'])\n",
    "    production_data = data['production_types']\n",
    "\n",
    "    df_list = []\n",
    "    for entry in production_data:\n",
    "        if entry['name'] == 'Hydro Run-of-River':\n",
    "            df = pd.DataFrame({\n",
    "                'Time': timestamps,\n",
    "                'Production Type': entry['name'],\n",
    "                'Power (MW)': entry['data']\n",
    "            })\n",
    "            df_list.append(df)\n",
    "\n",
    "    if not df_list:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Load data\n",
    "df = get_public_power()\n",
    "\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "df.set_index('Time', inplace=True)\n",
    "    \n",
    "\n",
    "data = df['Power (MW)'].values.reshape(-1, 1)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "print(df.head(25))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "source": [
    "df = df.dropna(how='any')\n",
    "print(df.head(25))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def create_dataset(data,\n",
    " look_back=1):\n",
    "    \"\"\"\n",
    "    Convert an array of values into a dataset matrix suitable for LSTM training.\n",
    "    \n",
    "    Parameters:\n",
    "        data (array): The dataset array containing sequential data.\n",
    "        look_back (int): Number of previous time steps to use as input variables to predict the next time period.\n",
    "        \n",
    "    Returns:\n",
    "        X (array): Feature dataset.\n",
    "        y (array): Labels.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        a = data[i:(i + look_back), 0]  # Fetch the input data from the current index to the look_back length\n",
    "        X.append(a)\n",
    "        y.append(data[i + look_back, 0])  # The label will be the next data point\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Assume 'data' is loaded and scaled as the 'Power (MW)' from your solar data.\n",
    "# For example purposes, let's define 'data' here manually:\n",
    "data = np.array([[i] for i in range(100)])  # Example data generation\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Define the look_back period\n",
    "look_back = 24  # This means the input for each sample will be sequences of 3 past observations\n",
    "\n",
    "# Create the dataset\n",
    "X, y = create_dataset(data, look_back)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "print(\"Sample of feature (X) and label (y):\")\n",
    "print(\"X[0]:\", X[0])\n",
    "print(\"y[0]:\", y[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "source": [
    "model = Sequential([\n",
    "     LSTM(60, input_shape=(look_back, 1)),\n",
    "     Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01) \n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "#callbacks=[early_stopping]\n",
    "history = model.fit(X, y, epochs=200, batch_size=16, validation_split=0.2, verbose=1)\n",
    "\n",
    "model.summary()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "source": [
    "\n",
    "# Plot training loss and validation loss\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "source": [
    "predictions = model.predict(X)\n",
    "\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "actuals = scaler.inverse_transform([y])\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(actuals[0], predictions[:,0]))\n",
    "mape = np.mean(np.abs((actuals[0] - predictions[:,0]) / actuals[0])) * 100\n",
    "r_squared = 1 - (np.sum((actuals[0] - predictions[:,0])**2) / np.sum((actuals[0] - np.mean(actuals[0]))**2))\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAPE: {mape}')\n",
    "print(f'R^2: {r_squared}')\n",
    "\n",
    "model.save('hydro_run_of_river_lstm.keras')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your trained model\n",
    "model = load_model('solar_power_lstm.keras')  # Update the path to where your model is saved\n",
    "\n",
    "# Scale and prepare the last 24 hours of data\n",
    "recent_data_scaled = scaler.transform(df['Power (MW)'].tail(24).values.reshape(-1, 1))\n",
    "recent_data_scaled = recent_data_scaled.reshape(1, 24, 1)\n",
    "\n",
    "predictions = []\n",
    "current_batch = recent_data_scaled\n",
    "\n",
    "# Predict iteratively\n",
    "for i in range(24):  # Predict the next 24 hours\n",
    "    current_pred = model.predict(current_batch)[0]  # Predict the next hour\n",
    "    predictions.append(current_pred)  # Store the prediction\n",
    "    current_batch = np.append(current_batch[:,1:,:], [[current_pred]], axis=1)  # Update the batch to include the new prediction\n",
    "\n",
    "# Inverse scale predictions\n",
    "predicted_original = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "\n",
    "# Print or store the predictions\n",
    "predicted_times = pd.date_range(start=df.index[-1] + pd.Timedelta(hours=1), periods=24, freq='H')\n",
    "predicted_df = pd.DataFrame(data=predicted_original.flatten(), index=predicted_times, columns=['Predicted Power (MW)'])\n",
    "print(predicted_df)\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
